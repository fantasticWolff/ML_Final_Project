{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s[] \n",
    "split no good\n",
    "lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "easy_ham = [f for f in listdir(\"data/easy_ham\") if isfile(join(\"data/easy_ham\", f))] # 2500 = 1716+784\n",
    "easy_ham_2 = [f for f in listdir(\"data/easy_ham_2\") if isfile(join(\"data/easy_ham_2\", f))] # 1400 = 960+440\n",
    "hard_ham = [f for f in listdir(\"data/hard_ham\") if isfile(join(\"data/hard_ham\", f))] # 250 = 172+78\n",
    "spam = [f for f in listdir(\"data/spam\") if isfile(join(\"data/spam\", f))] # 500 = 344+156\n",
    "spam_2 = [f for f in listdir(\"data/spam_2\") if isfile(join(\"data/spam_2\", f))] # 1396 = 958+438\n",
    "for index in range(len(easy_ham)):\n",
    "    easy_ham[index] = \"data/easy_ham/\"+ easy_ham[index]\n",
    "for index in range(len(easy_ham_2)):\n",
    "    easy_ham_2[index] = \"data/easy_ham_2/\"+ easy_ham_2[index]\n",
    "for index in range(len(hard_ham)):\n",
    "    hard_ham[index] = \"data/hard_ham/\"+ hard_ham[index]\n",
    "for index in range(len(spam)):\n",
    "    spam[index] = \"data/spam/\"+ spam[index]\n",
    "for index in range(len(spam_2)):\n",
    "    spam_2[index] = \"data/spam_2/\"+ spam_2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = easy_ham + easy_ham_2 + hard_ham\n",
    "spam = spam + spam_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Training_Ham, Testing_Ham = train_test_split(ham, test_size = 0.1, random_state = 1)\n",
    "Training_Spam, Testing_Spam = train_test_split(spam, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3735 415 1706 190\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_Ham), len(Testing_Ham), len(Training_Spam), len(Testing_Spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "# frequency_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "frequency_list = [10]\n",
    "\n",
    "dictionary = dict()\n",
    "Training = Training_Ham + Training_Spam\n",
    "Testing = Testing_Ham + Testing_Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency is 10\n",
      "dict length: 11610\n",
      "Spam\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "97.89\t\t97.89\t\t98.68\n",
      "\n",
      "Ham\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "99.04\t\t99.04\t\t98.68\n",
      "411 4 4 186\n",
      "\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "f = open(\"ret/NN_layer.txt\", \"a\")\n",
    "for frequency in frequency_list:\n",
    "    print('frequency is ' + str(frequency))\n",
    "    f.write('\\nfrequency is ' + str(frequency) + '\\n')\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for file_names in Training:\n",
    "        #f = open(file_names, \"r\")\n",
    "        ## UnicodeDecodeError: 'cp950' codec can't decode byte 0x93 in position 1885: illegal multibyte sequence\n",
    "        #f = open(file_names, \"r\", encoding=\"utf-8\")\n",
    "        ## UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 1885: invalid start byte\n",
    "\n",
    "        file = codecs.open(file_names, 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        #word = re.findall(r\"[\\w']+\", body) # 93783 ## 21525\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body) # 123656 ## 21224\n",
    "        for i in word:\n",
    "            dictionary[i] = dictionary.get(i, 0) + 1\n",
    "#     print(len(dictionary))\n",
    "    dictionary = {i:dictionary[i] for i in dictionary if dictionary[i]>frequency}\n",
    "#     print(len(dictionary))\n",
    "    #print(dictionary)\n",
    "    list_dictionary = list(dictionary)\n",
    "    list_dictionary = list(filter(lambda w: not w in s,list_dictionary))\n",
    "    \n",
    "    # Remove common word in dictionary\n",
    "    for s__ in s:\n",
    "        if s__ in dictionary:\n",
    "            del dictionary[s__]\n",
    "\n",
    "    print('dict length: ' + str(len(dictionary)))\n",
    "    f.write('dict length: ' + str(len(dictionary)))\n",
    "    count = 0\n",
    "    for i in dictionary:\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "\n",
    "    train_ndarray = np.zeros((len(Training), len(dictionary)), dtype = int)\n",
    "\n",
    "    label_train = [0] * len(Training_Ham) + [1] * len(Training_Spam)\n",
    "    label_test = [0] * len(Testing_Ham) + [1] * len(Testing_Spam)\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Training_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Training_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    train_dataframe = pd.DataFrame(train_ndarray)\n",
    "\n",
    "    test_ndarray = np.zeros((len(Testing), len(dictionary)), dtype = int)\n",
    "    test_ndarray.shape\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Testing_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Testing_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    test_dataframe = pd.DataFrame(test_ndarray)\n",
    "\n",
    "#     NN = MLPClassifier(hidden_layer_sizes=(5,3))\n",
    "    NN = MLPClassifier()\n",
    "\n",
    "    NN.fit(train_dataframe, label_train)\n",
    "\n",
    "    result = NN.predict(test_dataframe)\n",
    "    result_prob = NN.predict_proba(test_dataframe)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #-------------- from block -----------------\n",
    "    \n",
    "#     Spam_from = dict()\n",
    "#     count = 0\n",
    "#     for Spam_mail in Training_Spam:\n",
    "#         file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#         word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#         if not word_2 :\n",
    "#             continue\n",
    "#         Spam_from[word_2[0]] = Spam_from.get(i, 0)    \n",
    "    \n",
    "#     index = 0\n",
    "#     cheat_result = result.copy()\n",
    "#     for mail in Testing:\n",
    "#         file = codecs.open(mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#         word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#         if word_2[0] in Spam_from:\n",
    "#             cheat_result[index] = 1\n",
    "#         index += 1\n",
    "    \n",
    "#     eva_fun_2(label_test, cheat_result, 'NN')\n",
    "    \n",
    "    #--------------- end ---------------------\n",
    "    \n",
    "     \n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, result).ravel()\n",
    "\n",
    "    Accuracy = (tp+tn)/(tn+ fp+ fn+ tp)\n",
    "    spam_Precision = (tp)/(fp+ tp)\n",
    "    spam_Recall = (tp)/(fn+ tp)\n",
    "    \n",
    "    ham_Precision = (tn)/(fn + tn)\n",
    "    ham_Recall = (tn)/(fp + tn)\n",
    "    \n",
    "    print('Spam\\nRecall\\t\\tPrecision\\tAccuracy')\n",
    "    f.write('\\n\\nSpam\\nRecall\\t\\tPrecision\\tAccuracy\\n')\n",
    "    print(str(round(spam_Recall*100, 2))+'\\t\\t'+ str(round(spam_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    f.write(str(round(spam_Recall*100, 2))+'\\t\\t'+ str(round(spam_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    \n",
    "    print('\\nHam\\nRecall\\t\\tPrecision\\tAccuracy')\n",
    "    f.write('\\n\\nHam\\nRecall\\t\\tPrecision\\tAccuracy\\n')\n",
    "    print(str(round(ham_Recall*100, 2))+'\\t\\t'+ str(round(ham_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    f.write(str(round(ham_Recall*100, 2))+'\\t\\t'+ str(round(ham_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2))+ '\\n\\n')\n",
    "    \n",
    "    print(tn, fp, fn, tp)\n",
    "    f.write('tn ' + str(tn) + ' fp ' + str(fp) + ' fn ' + str(fn) + ' tp ' + str(tp))\n",
    "    print('\\n------------------------------------\\n')\n",
    "    f.write('\\n------------------------------------\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n",
      "[0.38740253 0.61259747]\n",
      "602\n",
      "[0.35970341 0.64029659]\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result_prob)):\n",
    "    if (result_prob[i][0] > 0.3) and (result_prob[i][1] > 0.3):\n",
    "        print(i)\n",
    "        print(result_prob[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
