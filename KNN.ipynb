{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s = [] very good\n",
    "split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "easy_ham = [f for f in listdir(\"data/easy_ham\") if isfile(join(\"data/easy_ham\", f))] # 2500 = 1716+784\n",
    "easy_ham_2 = [f for f in listdir(\"data/easy_ham_2\") if isfile(join(\"data/easy_ham_2\", f))] # 1400 = 960+440\n",
    "hard_ham = [f for f in listdir(\"data/hard_ham\") if isfile(join(\"data/hard_ham\", f))] # 250 = 172+78\n",
    "spam = [f for f in listdir(\"data/spam\") if isfile(join(\"data/spam\", f))] # 500 = 344+156\n",
    "spam_2 = [f for f in listdir(\"data/spam_2\") if isfile(join(\"data/spam_2\", f))] # 1396 = 958+438\n",
    "for index in range(len(easy_ham)):\n",
    "    easy_ham[index] = \"data/easy_ham/\"+ easy_ham[index]\n",
    "for index in range(len(easy_ham_2)):\n",
    "    easy_ham_2[index] = \"data/easy_ham_2/\"+ easy_ham_2[index]\n",
    "for index in range(len(hard_ham)):\n",
    "    hard_ham[index] = \"data/hard_ham/\"+ hard_ham[index]\n",
    "for index in range(len(spam)):\n",
    "    spam[index] = \"data/spam/\"+ spam[index]\n",
    "for index in range(len(spam_2)):\n",
    "    spam_2[index] = \"data/spam_2/\"+ spam_2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = easy_ham + easy_ham_2 + hard_ham\n",
    "spam = spam + spam_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Training_Ham, Testing_Ham = train_test_split(ham, test_size = 0.1, random_state = 1)\n",
    "Training_Spam, Testing_Spam = train_test_split(spam, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3735 415 1706 190\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_Ham), len(Testing_Ham), len(Training_Spam), len(Testing_Spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))\n",
    "# s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "frequency_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300]\n",
    "# frequency_list = [3, 50, 1000]\n",
    "dictionary = dict()\n",
    "Training = Training_Ham + Training_Spam\n",
    "Testing = Testing_Ham + Testing_Spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency is 1\n",
      "dict length: 83608\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "93.68\t\t63.12\t\t80.83\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 2\n",
      "dict length: 48665\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "94.74\t\t63.16\t\t80.99\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 3\n",
      "dict length: 36475\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "94.21\t\t62.37\t\t80.33\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 5\n",
      "dict length: 24595\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "94.21\t\t63.03\t\t80.83\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 10\n",
      "dict length: 12954\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "93.16\t\t65.8\t\t82.64\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 20\n",
      "dict length: 7319\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "83.68\t\t73.95\t\t85.62\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 30\n",
      "dict length: 5031\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "82.63\t\t71.36\t\t84.13\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 50\n",
      "dict length: 3140\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "78.95\t\t72.46\t\t83.97\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 100\n",
      "dict length: 1560\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "77.37\t\t73.13\t\t83.97\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 200\n",
      "dict length: 689\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "71.05\t\t82.32\t\t86.12\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 300\n",
      "dict length: 429\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "69.47\t\t89.19\t\t87.77\n",
      "\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "f = open(\"ret/KNN.txt\", \"a\")\n",
    "for frequency in frequency_list:\n",
    "    print('frequency is ' + str(frequency))\n",
    "    f.write('\\nfrequency is ' + str(frequency) + '\\n')\n",
    "    dictionary = dict()\n",
    "\n",
    "    for file_names in Training:\n",
    "        #f = open(file_names, \"r\")\n",
    "        ## UnicodeDecodeError: 'cp950' codec can't decode byte 0x93 in position 1885: illegal multibyte sequence\n",
    "        #f = open(file_names, \"r\", encoding=\"utf-8\")\n",
    "        ## UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 1885: invalid start byte\n",
    "\n",
    "        file = codecs.open(file_names, 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body) # 123656 ## 21224\n",
    "        word = body.split(' ')\n",
    "        for i in word:\n",
    "            dictionary[i] = dictionary.get(i, 0) + 1\n",
    "#     print(len(dictionary))\n",
    "    dictionary = {i:dictionary[i] for i in dictionary if dictionary[i]>frequency}\n",
    "#     print(len(dictionary))\n",
    "    #print(dictionary)\n",
    "    list_dictionary = list(dictionary)\n",
    "    list_dictionary = list(filter(lambda w: not w in s,list_dictionary))\n",
    "    list_dictionary\n",
    "#     print(len(list_dictionary))\n",
    "\n",
    "    # Remove common word in dictionary\n",
    "    for s__ in s:\n",
    "        if s__ in dictionary:\n",
    "            del dictionary[s__]\n",
    "    print('dict length: ' + str(len(dictionary)))\n",
    "    f.write('dict length: ' + str(len(dictionary)))\n",
    "    count = 0\n",
    "    for i in dictionary:\n",
    "    #     print(i)\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "    dictionary\n",
    "\n",
    "    train_ndarray = np.zeros((len(Training), len(dictionary)), dtype = int)\n",
    "    train_ndarray.shape\n",
    "\n",
    "    label_train = [0] * len(Training_Ham) + [1] * len(Training_Spam)\n",
    "    label_test = [0] * len(Testing_Ham) + [1] * len(Testing_Spam)\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Training_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    for Spam_mail in Training_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    train_dataframe = pd.DataFrame(train_ndarray)\n",
    "\n",
    "    test_ndarray = np.zeros((len(Testing), len(dictionary)), dtype = int)\n",
    "    test_ndarray.shape\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Testing_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Testing_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    test_dataframe = pd.DataFrame(test_ndarray)\n",
    "\n",
    "    neigh = KNeighborsClassifier(n_neighbors = 3)\n",
    "    neigh.fit(train_dataframe, label_train)\n",
    "\n",
    "    result = neigh.predict(test_dataframe)\n",
    "\n",
    "\n",
    "    #-------------- from block -----------------\n",
    "\n",
    "#     Spam_from = dict()\n",
    "#     count = 0\n",
    "#     for Spam_mail in Training_Spam:\n",
    "#         file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#         word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#         if not word_2 :\n",
    "#             continue\n",
    "#         Spam_from[word_2[0]] = Spam_from.get(i, 0)    \n",
    "\n",
    "#     index = 0\n",
    "#     cheat_result = result.copy()\n",
    "#     for mail in Testing:\n",
    "#         file = codecs.open(mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#         word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#         if word_2[0] in Spam_from:\n",
    "#             cheat_result[index] = 1\n",
    "#         index += 1\n",
    "\n",
    "#     eva_fun_2(label_test, cheat_result, 'NN')\n",
    "\n",
    "    #--------------- end ---------------------\n",
    "\n",
    "#     print('neighbor is :' + str(nn) + '\\n')\n",
    "#     f.write('neighbor is :' + str(nn) + '\\n')\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, result).ravel()\n",
    "\n",
    "    Accuracy = (tp+tn)/(tn+ fp+ fn+ tp)\n",
    "    Precision = (tp)/(fp+ tp)\n",
    "    Recall = (tp)/(fn+ tp)\n",
    "    print('Recall\\t\\tPrecision\\tAccuracy')\n",
    "    f.write('\\nRecall\\t\\tPrecision\\tAccuracy\\n')\n",
    "    print(str(round(Recall*100, 2))+'\\t\\t'+ str(round(Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    f.write(str(round(Recall*100, 2))+'\\t\\t'+ str(round(Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    print('\\n------------------------------------\\n')\n",
    "    f.write('\\n------------------------------------\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "429"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# cheat_result = result.copy()\n",
    "# for mail in Testing:\n",
    "#     file = codecs.open(mail, 'r', encoding='utf-8', errors='ignore')\n",
    "# #     print(file.read())\n",
    "#     word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "# #     print(Spam_mail)\n",
    "#     word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "# #     if word_2[0] == \"admin@xent.com\":\n",
    "# #         continue\n",
    "# #     if word_2[0] == \"admin@lists.sourceforge.net\":\n",
    "# #         continue\n",
    "# #     if word_2[0] == \"admin@linux.ie\":\n",
    "# #         continue\n",
    "#     if word_2[0] in Spam_from:\n",
    "#         cheat_result[count] = 1\n",
    "#         print(word_2[0])\n",
    "# #     print(word_2[0])\n",
    "#     count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
