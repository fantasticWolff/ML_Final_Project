{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "easy_ham = [f for f in listdir(\"data/easy_ham\") if isfile(join(\"data/easy_ham\", f))] # 2500 = 1716+784\n",
    "easy_ham_2 = [f for f in listdir(\"data/easy_ham_2\") if isfile(join(\"data/easy_ham_2\", f))] # 1400 = 960+440\n",
    "hard_ham = [f for f in listdir(\"data/hard_ham\") if isfile(join(\"data/hard_ham\", f))] # 250 = 172+78\n",
    "spam = [f for f in listdir(\"data/spam\") if isfile(join(\"data/spam\", f))] # 500 = 344+156\n",
    "spam_2 = [f for f in listdir(\"data/spam_2\") if isfile(join(\"data/spam_2\", f))] # 1396 = 958+438\n",
    "for index in range(len(easy_ham)):\n",
    "    easy_ham[index] = \"data/easy_ham/\"+ easy_ham[index]\n",
    "for index in range(len(easy_ham_2)):\n",
    "    easy_ham_2[index] = \"data/easy_ham_2/\"+ easy_ham_2[index]\n",
    "for index in range(len(hard_ham)):\n",
    "    hard_ham[index] = \"data/hard_ham/\"+ hard_ham[index]\n",
    "for index in range(len(spam)):\n",
    "    spam[index] = \"data/spam/\"+ spam[index]\n",
    "for index in range(len(spam_2)):\n",
    "    spam_2[index] = \"data/spam_2/\"+ spam_2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2848"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training_Ham = easy_ham[0:1716]\n",
    "Training_Ham += easy_ham_2[0:960]\n",
    "Training_Ham += hard_ham[0:172]\n",
    "len(Training_Ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1299"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_Ham = easy_ham[1716+1:]\n",
    "Testing_Ham += easy_ham_2[960+1:]\n",
    "Testing_Ham += hard_ham[172+1:]\n",
    "len(Testing_Ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1302"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Training_Spam = spam[0:344]\n",
    "Training_Spam += spam_2[0:958]\n",
    "len(Training_Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "592"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testing_Spam = spam[344+1:]\n",
    "Testing_Spam += spam_2[958+1:]\n",
    "len(Testing_Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "frequency_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300, 500, 1000, 2000, 10000]\n",
    "# frequency_list = [10000]\n",
    "\n",
    "dictionary = dict()\n",
    "Training = Training_Ham + Training_Spam\n",
    "Testing = Testing_Ham + Testing_Spam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency is 1\n",
      "dict length: 43861\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.79\t\t94.4\t\t97.2\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 2\n",
      "dict length: 26435\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "97.13\t\t94.57\t\t97.36\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 3\n",
      "dict length: 21068\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.79\t\t94.55\t\t97.25\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 5\n",
      "dict length: 15425\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.62\t\t94.39\t\t97.14\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 10\n",
      "dict length: 9890\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.62\t\t94.55\t\t97.2\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 20\n",
      "dict length: 6247\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.79\t\t94.71\t\t97.3\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 30\n",
      "dict length: 4690\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.79\t\t94.55\t\t97.25\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 50\n",
      "dict length: 3170\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.79\t\t94.55\t\t97.25\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 100\n",
      "dict length: 1746\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "96.28\t\t93.29\t\t96.67\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 200\n",
      "dict length: 923\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "95.27\t\t92.46\t\t96.09\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 300\n",
      "dict length: 618\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "94.26\t\t92.54\t\t95.82\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 500\n",
      "dict length: 364\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "91.39\t\t89.13\t\t93.81\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 1000\n",
      "dict length: 170\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "90.03\t\t83.41\t\t91.27\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 2000\n",
      "dict length: 89\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "78.89\t\t83.99\t\t88.68\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 10000\n",
      "dict length: 18\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "58.61\t\t87.41\t\t84.4\n",
      "\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "f = open(\"ret/SVM.txt\", \"a\")\n",
    "for frequency in frequency_list:\n",
    "    print('frequency is ' + str(frequency))\n",
    "    f.write('\\nfrequency is ' + str(frequency) + '\\n')\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for file_names in Training:\n",
    "        #f = open(file_names, \"r\")\n",
    "        ## UnicodeDecodeError: 'cp950' codec can't decode byte 0x93 in position 1885: illegal multibyte sequence\n",
    "        #f = open(file_names, \"r\", encoding=\"utf-8\")\n",
    "        ## UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 1885: invalid start byte\n",
    "\n",
    "        file = codecs.open(file_names, 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        #word = re.findall(r\"[\\w']+\", body) # 93783 ## 21525\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body) # 123656 ## 21224\n",
    "        for i in word:\n",
    "            dictionary[i] = dictionary.get(i, 0) + 1\n",
    "#     print(len(dictionary))\n",
    "    dictionary = {i:dictionary[i] for i in dictionary if dictionary[i]>frequency}\n",
    "#     print(len(dictionary))\n",
    "    #print(dictionary)\n",
    "    list_dictionary = list(dictionary)\n",
    "    list_dictionary = list(filter(lambda w: not w in s,list_dictionary))\n",
    "    list_dictionary\n",
    "#     print(len(list_dictionary))\n",
    "    \n",
    "    # Remove common word in dictionary\n",
    "    for s__ in s:\n",
    "        if s__ in dictionary:\n",
    "            del dictionary[s__]\n",
    "#     print(len(dictionary))\n",
    "\n",
    "    print('dict length: ' + str(len(dictionary)))\n",
    "    f.write('dict length: ' + str(len(dictionary)))\n",
    "    count = 0\n",
    "    for i in dictionary:\n",
    "    #     print(i)\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "    dictionary\n",
    "\n",
    "    # ****************** train *****************\n",
    "    train_ndarray = np.zeros((len(Training), len(dictionary)), dtype = float)\n",
    "    train_ndarray.shape\n",
    "\n",
    "    label_train = [0] * len(Training_Ham) + [1] * len(Training_Spam)\n",
    "    label_test = [0] * len(Testing_Ham) + [1] * len(Testing_Spam)\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Training_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "#         dict_count = 0\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "#                 dict_count += 1\n",
    "#         train_ndarray[count][-1] = dict_count/len(word)\n",
    "        count += 1\n",
    "        \n",
    "    for Spam_mail in Training_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "#         dict_count = 0\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "#                 dict_count += 1\n",
    "#         train_ndarray[count][-1] = dict_count/len(word)\n",
    "        count += 1\n",
    "    train_dataframe = pd.DataFrame(train_ndarray)\n",
    "#     train_dataframe.rename(columns={len(dictionary):'whole_dict_propotion'})\n",
    "    \n",
    "# ****************** test ******************\n",
    "    \n",
    "    test_ndarray = np.zeros((len(Testing), len(dictionary)), dtype = float)\n",
    "    test_ndarray.shape\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Testing_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "#         dict_count = 0\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "#                 dict_count += 1\n",
    "#         test_ndarray[count][-1] = dict_count/len(word)\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Testing_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "#         dict_count = 0\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "#                 dict_count += 1\n",
    "#         test_ndarray[count][-1] = dict_count/len(word)\n",
    "        count += 1\n",
    "    test_dataframe = pd.DataFrame(test_ndarray)\n",
    "#     test_dataframe.rename(columns={len(dictionary):'whole_dict_propotion'})\n",
    "    \n",
    "# **************************************************************\n",
    "\n",
    "    SVM = SVC(kernel='linear')\n",
    "    SVM.fit(train_dataframe, label_train)\n",
    "\n",
    "    result = SVM.predict(test_dataframe)\n",
    "\n",
    "    \n",
    "    #-------------- from block -----------------\n",
    "    \n",
    "#     Spam_from = dict()\n",
    "#     count = 0\n",
    "#     for Spam_mail in Training_Spam:\n",
    "#         file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#         word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#         if not word_2 :\n",
    "#             continue\n",
    "#         Spam_from[word_2[0]] = Spam_from.get(i, 0)    \n",
    "    \n",
    "#     index = 0\n",
    "#     cheat_result = result.copy()\n",
    "#     for mail in Testing:\n",
    "#         file = codecs.open(mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#         word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#         if word_2[0] in Spam_from:\n",
    "#             cheat_result[index] = 1\n",
    "#         index += 1\n",
    "    \n",
    "#     eva_fun_2(label_test, cheat_result, 'NN')\n",
    "    \n",
    "    #--------------- end ---------------------\n",
    "    \n",
    "     \n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, result).ravel()\n",
    "\n",
    "    Accuracy = (tp+tn)/(tn+ fp+ fn+ tp)\n",
    "    Precision = (tp)/(fp+ tp)\n",
    "    Recall = (tp)/(fn+ tp)\n",
    "    print('Recall\\t\\tPrecision\\tAccuracy')\n",
    "    f.write('\\nRecall\\t\\tPrecision\\tAccuracy\\n')\n",
    "    print(str(round(Recall*100, 2))+'\\t\\t'+ str(round(Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    f.write(str(round(Recall*100, 2))+'\\t\\t'+ str(round(Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    print('\\n------------------------------------\\n')\n",
    "    f.write('\\n------------------------------------\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jun 12 15:40:55 2020\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    " \n",
    "localtime = time.asctime( time.localtime(time.time()) )\n",
    "print(localtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "# Remove common word in dictionary\n",
    "for s__ in s:\n",
    "    if s__ in dictionary:\n",
    "        del dictionary[s__]\n",
    "print(len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'com': 0,\n",
       " 'p': 1,\n",
       " 'http': 2,\n",
       " 'www': 3,\n",
       " 'face': 4,\n",
       " 'width': 5,\n",
       " 'b': 6,\n",
       " 'size': 7,\n",
       " 'font': 8,\n",
       " 'nbsp': 9,\n",
       " 'br': 10,\n",
       " 'href': 11,\n",
       " 'gif': 12,\n",
       " 'src': 13,\n",
       " 'tr': 14,\n",
       " 'td': 15,\n",
       " 'img': 16,\n",
       " 'height': 17}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for i in dictionary:\n",
    "#     print(i)\n",
    "    dictionary[i] = count\n",
    "    count += 1\n",
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4150, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "train_ndarray = np.zeros((len(Training), len(dictionary)), dtype = int)\n",
    "train_ndarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_train = [0] * len(Training_Ham) + [1] * len(Training_Spam)\n",
    "label_test = [0] * len(Testing_Ham) + [1] * len(Testing_Spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count = 0\n",
    "for Ham_mail in Training_Ham:\n",
    "    file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "    body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "    word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "    for w in word:\n",
    "        if w in dictionary:\n",
    "            train_ndarray[count][dictionary[w]] = 1\n",
    "#     print(train_ndarray[count])\n",
    "    count += 1\n",
    "for Spam_mail in Training_Spam:\n",
    "    file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "    body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "    word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "    for w in word:\n",
    "        if w in dictionary:\n",
    "            train_ndarray[count][dictionary[w]] = 1\n",
    "    count += 1\n",
    "train_dataframe = pd.DataFrame(train_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1891, 18)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "test_ndarray = np.zeros((len(Testing), len(dictionary)), dtype = int)\n",
    "test_ndarray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "count = 0\n",
    "for Ham_mail in Testing_Ham:\n",
    "    file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "    body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "    word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "    for w in word:\n",
    "        if w in dictionary:\n",
    "            test_ndarray[count][dictionary[w]] = 1\n",
    "#     print(train_ndarray[count])\n",
    "    count += 1\n",
    "for Spam_mail in Testing_Spam:\n",
    "    file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "    body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "    word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "    for w in word:\n",
    "        if w in dictionary:\n",
    "            test_ndarray[count][dictionary[w]] = 1\n",
    "    count += 1\n",
    "test_dataframe = pd.DataFrame(test_ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=5)\n",
    "neigh.fit(train_dataframe, label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = neigh.predict(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eva_fun_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-e03b750f0cfc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0meva_fun_2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'K-nn'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'eva_fun_2' is not defined"
     ]
    }
   ],
   "source": [
    "eva_fun_2(label_test, result, 'K-nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for Spam_mail in Training_Spam:\n",
    "    file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#     print(file.read())\n",
    "    word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#     print(Spam_mail)\n",
    "    word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "    if not word_2 :\n",
    "        continue\n",
    "    Spam_from[word_2[0]] = Spam_from.get(i, 0)\n",
    "#     print(word_2[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Spam_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "cheat_result = result.copy()\n",
    "for mail in Testing:\n",
    "    file = codecs.open(mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#     print(file.read())\n",
    "    word = re.findall(r\"From.*\\n\", file.read())[0]\n",
    "#     print(Spam_mail)\n",
    "    word_2 = re.findall(r\"[\\w*\\.?]*@[\\w*\\.?]*\", word)\n",
    "#     if word_2[0] == \"admin@xent.com\":\n",
    "#         continue\n",
    "#     if word_2[0] == \"admin@lists.sourceforge.net\":\n",
    "#         continue\n",
    "#     if word_2[0] == \"admin@linux.ie\":\n",
    "#         continue\n",
    "    if word_2[0] in Spam_from:\n",
    "        cheat_result[count] = 1\n",
    "        print(word_2[0])\n",
    "#     print(word_2[0])\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cheat_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eva_fun_2(label_test, cheat_result, 'K-nn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(cheat_result)):\n",
    "    if cheat_result[i] != result[i]:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
