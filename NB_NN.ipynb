{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s[]\n",
    "word=body.split(' ')\n",
    "lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "easy_ham = [f for f in listdir(\"data/easy_ham\") if isfile(join(\"data/easy_ham\", f))] # 2500 = 1716+784\n",
    "easy_ham_2 = [f for f in listdir(\"data/easy_ham_2\") if isfile(join(\"data/easy_ham_2\", f))] # 1400 = 960+440\n",
    "hard_ham = [f for f in listdir(\"data/hard_ham\") if isfile(join(\"data/hard_ham\", f))] # 250 = 172+78\n",
    "spam = [f for f in listdir(\"data/spam\") if isfile(join(\"data/spam\", f))] # 500 = 344+156\n",
    "spam_2 = [f for f in listdir(\"data/spam_2\") if isfile(join(\"data/spam_2\", f))] # 1396 = 958+438\n",
    "for index in range(len(easy_ham)):\n",
    "    easy_ham[index] = \"data/easy_ham/\"+ easy_ham[index]\n",
    "for index in range(len(easy_ham_2)):\n",
    "    easy_ham_2[index] = \"data/easy_ham_2/\"+ easy_ham_2[index]\n",
    "for index in range(len(hard_ham)):\n",
    "    hard_ham[index] = \"data/hard_ham/\"+ hard_ham[index]\n",
    "for index in range(len(spam)):\n",
    "    spam[index] = \"data/spam/\"+ spam[index]\n",
    "for index in range(len(spam_2)):\n",
    "    spam_2[index] = \"data/spam_2/\"+ spam_2[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham = easy_ham + easy_ham_2 + hard_ham\n",
    "spam = spam + spam_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Training_Ham, Testing_Ham = train_test_split(ham, test_size = 0.1, random_state = 1)\n",
    "Training_Spam, Testing_Spam = train_test_split(spam, test_size = 0.1, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3735 415 1706 190\n"
     ]
    }
   ],
   "source": [
    "print(len(Training_Ham), len(Testing_Ham), len(Training_Spam), len(Testing_Spam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "s=set(stopwords.words('english'))\n",
    "# s=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import codecs\n",
    "# frequency_list = [1, 2, 3, 5, 10, 20, 30, 50, 100, 200, 300, 500, 1000, 2000, 10000]\n",
    "frequency_list = [1, 2, 3, 5]\n",
    "\n",
    "dictionary = dict()\n",
    "Training = Training_Ham + Training_Spam\n",
    "Testing = Testing_Ham + Testing_Spam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frequency is 1\n",
      "dict length: 114060\n",
      "Spam\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "97.89\t\t97.89\t\t98.68\n",
      "\n",
      "Ham\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "99.04\t\t99.04\t\t98.68\n",
      "411 4 4 186\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 2\n",
      "dict length: 64654\n",
      "Spam\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "97.89\t\t97.89\t\t98.68\n",
      "\n",
      "Ham\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "99.04\t\t99.04\t\t98.68\n",
      "411 4 4 186\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 3\n",
      "dict length: 47130\n",
      "Spam\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "98.42\t\t97.91\t\t98.84\n",
      "\n",
      "Ham\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "99.04\t\t99.28\t\t98.84\n",
      "411 4 3 187\n",
      "\n",
      "------------------------------------\n",
      "\n",
      "frequency is 5\n",
      "dict length: 31202\n",
      "Spam\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "97.89\t\t97.89\t\t98.68\n",
      "\n",
      "Ham\n",
      "Recall\t\tPrecision\tAccuracy\n",
      "99.04\t\t99.04\t\t98.68\n",
      "411 4 4 186\n",
      "\n",
      "------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#----------------------------------------------- NB -----------------------------------------------\n",
    "\n",
    "f = open(\"ret/NB_NN_0.1.txt\", \"a\")\n",
    "for frequency in frequency_list:\n",
    "    print('frequency is ' + str(frequency))\n",
    "    f.write('\\nfrequency is ' + str(frequency) + '\\n')\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for file_names in Training:\n",
    "        #f = open(file_names, \"r\")\n",
    "        ## UnicodeDecodeError: 'cp950' codec can't decode byte 0x93 in position 1885: illegal multibyte sequence\n",
    "        #f = open(file_names, \"r\", encoding=\"utf-8\")\n",
    "        ## UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 1885: invalid start byte\n",
    "\n",
    "        file = codecs.open(file_names, 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "#         body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        body = file.read()\n",
    "        #word = re.findall(r\"[\\w']+\", body) # 93783 ## 21525\n",
    "#         word = re.findall(r\"[a-zA-Z']+\", body) # 123656 ## 21224\n",
    "#         token = body.split()\n",
    "#         word = []\n",
    "#         for string in token:\n",
    "#             word += re.split(r'[`\\-=~!@#$%^&*()_+\\[\\]{};\\'\\\\:\"|<,./<>?]', string)\n",
    "        word = body.split(' ')\n",
    "        for i in word:\n",
    "            dictionary[i] = dictionary.get(i, 0) + 1\n",
    "#     print(len(dictionary))\n",
    "    dictionary = {i:dictionary[i] for i in dictionary if dictionary[i]>frequency}\n",
    "#     print(len(dictionary))\n",
    "    #print(dictionary)\n",
    "    list_dictionary = list(dictionary)\n",
    "    list_dictionary = list(filter(lambda w: not w in [],list_dictionary))\n",
    "    list_dictionary\n",
    "#     print(len(list_dictionary))\n",
    "    \n",
    "    # Remove common word in dictionary\n",
    "    for s__ in s:\n",
    "        if s__ in dictionary:\n",
    "            del dictionary[s__]\n",
    "#     print(len(dictionary))\n",
    "\n",
    "    print('dict length: ' + str(len(dictionary)))\n",
    "    f.write('dict length: ' + str(len(dictionary)))\n",
    "    count = 0\n",
    "    for i in dictionary:\n",
    "    #     print(i)\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "    dictionary\n",
    "\n",
    "    train_ndarray = np.zeros((len(Training), len(dictionary)), dtype = int)\n",
    "    train_ndarray.shape\n",
    "\n",
    "    label_train = [0] * len(Training_Ham) + [1] * len(Training_Spam)\n",
    "    label_test = [0] * len(Testing_Ham) + [1] * len(Testing_Spam)\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Training_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        body = file.read()\n",
    "#         word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Training_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        body = file.read()\n",
    "\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    train_dataframe = pd.DataFrame(train_ndarray)\n",
    "\n",
    "    test_ndarray = np.zeros((len(Testing), len(dictionary)), dtype = int)\n",
    "    test_ndarray.shape\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Testing_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        body = file.read()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Testing_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "#         body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        body = file.read()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        word = body.split(' ')\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    test_dataframe = pd.DataFrame(test_ndarray)\n",
    "\n",
    "    NB = GaussianNB()\n",
    "    NB.fit(train_dataframe, label_train)\n",
    "\n",
    "    NB_result = NB.predict(test_dataframe)\n",
    "\n",
    "    \n",
    "#----------------------------------------------- NN -----------------------------------------------\n",
    "    dictionary = dict()\n",
    "    \n",
    "    for file_names in Training:\n",
    "        #f = open(file_names, \"r\")\n",
    "        ## UnicodeDecodeError: 'cp950' codec can't decode byte 0x93 in position 1885: illegal multibyte sequence\n",
    "        #f = open(file_names, \"r\", encoding=\"utf-8\")\n",
    "        ## UnicodeDecodeError: 'utf-8' codec can't decode byte 0x93 in position 1885: invalid start byte\n",
    "\n",
    "        file = codecs.open(file_names, 'r', encoding='utf-8', errors='ignore')\n",
    "\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        #word = re.findall(r\"[\\w']+\", body) # 93783 ## 21525\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body) # 123656 ## 21224\n",
    "        for i in word:\n",
    "            dictionary[i] = dictionary.get(i, 0) + 1\n",
    "#     print(len(dictionary))\n",
    "    dictionary = {i:dictionary[i] for i in dictionary if dictionary[i]>frequency}\n",
    "#     print(len(dictionary))\n",
    "    #print(dictionary)\n",
    "    list_dictionary = list(dictionary)\n",
    "    list_dictionary = list(filter(lambda w: not w in s,list_dictionary))\n",
    "    list_dictionary\n",
    "#     print(len(list_dictionary))\n",
    "    \n",
    "    # Remove common word in dictionary\n",
    "    for s__ in s:\n",
    "        if s__ in dictionary:\n",
    "            del dictionary[s__]\n",
    "#     print(len(dictionary))\n",
    "\n",
    "#     print('dict length: ' + str(len(dictionary)))\n",
    "#     f.write('dict length: ' + str(len(dictionary)))\n",
    "    count = 0\n",
    "    for i in dictionary:\n",
    "    #     print(i)\n",
    "        dictionary[i] = count\n",
    "        count += 1\n",
    "    dictionary\n",
    "\n",
    "    train_ndarray = np.zeros((len(Training), len(dictionary)), dtype = int)\n",
    "    train_ndarray.shape\n",
    "\n",
    "    label_train = [0] * len(Training_Ham) + [1] * len(Training_Spam)\n",
    "    label_test = [0] * len(Testing_Ham) + [1] * len(Testing_Spam)\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Training_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Training_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                train_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    train_dataframe = pd.DataFrame(train_ndarray)\n",
    "\n",
    "    test_ndarray = np.zeros((len(Testing), len(dictionary)), dtype = int)\n",
    "    test_ndarray.shape\n",
    "\n",
    "    count = 0\n",
    "    for Ham_mail in Testing_Ham:\n",
    "        file = codecs.open(Ham_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "    #     print(train_ndarray[count])\n",
    "        count += 1\n",
    "    for Spam_mail in Testing_Spam:\n",
    "        file = codecs.open(Spam_mail, 'r', encoding='utf-8', errors='ignore')\n",
    "        body = file.read().split(\"\\n\\n\", 1)[1].lower()\n",
    "        word = re.findall(r\"[a-zA-Z']+\", body)\n",
    "        for w in word:\n",
    "            if w in dictionary:\n",
    "                test_ndarray[count][dictionary[w]] = 1\n",
    "        count += 1\n",
    "    test_dataframe = pd.DataFrame(test_ndarray)\n",
    "\n",
    "    NN = MLPClassifier()\n",
    "    NN.fit(train_dataframe, label_train)\n",
    "\n",
    "    NN_result = NN.predict(test_dataframe)\n",
    "    NN_result_prob = NN.predict_proba(test_dataframe)\n",
    "    \n",
    "    for i in range(len(NN_result)):\n",
    "        if (NB_result[i] == 1) and (NN_result_prob[i][0] > 0.4) and (NN_result_prob[i][1] > 0.4):\n",
    "            NN_result[i] = 1\n",
    "#             print(i)\n",
    " \n",
    "    tn, fp, fn, tp = confusion_matrix(label_test, NN_result).ravel()\n",
    "\n",
    "    Accuracy = (tp+tn)/(tn+ fp+ fn+ tp)\n",
    "    spam_Precision = (tp)/(fp+ tp)\n",
    "    spam_Recall = (tp)/(fn+ tp)\n",
    "    \n",
    "    ham_Precision = (tn)/(fn + tn)\n",
    "    ham_Recall = (tn)/(fp + tn)\n",
    "    \n",
    "    print('Spam\\nRecall\\t\\tPrecision\\tAccuracy')\n",
    "    f.write('\\n\\nSpam\\nRecall\\t\\tPrecision\\tAccuracy\\n')\n",
    "    print(str(round(spam_Recall*100, 2))+'\\t\\t'+ str(round(spam_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    f.write(str(round(spam_Recall*100, 2))+'\\t\\t'+ str(round(spam_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    \n",
    "    print('\\nHam\\nRecall\\t\\tPrecision\\tAccuracy')\n",
    "    f.write('\\n\\nHam\\nRecall\\t\\tPrecision\\tAccuracy\\n')\n",
    "    print(str(round(ham_Recall*100, 2))+'\\t\\t'+ str(round(ham_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2)))\n",
    "    f.write(str(round(ham_Recall*100, 2))+'\\t\\t'+ str(round(ham_Precision*100, 2))+'\\t\\t'+str(round(Accuracy*100, 2))+ '\\n\\n')\n",
    "    \n",
    "    print(tn, fp, fn, tp)\n",
    "    f.write('tn ' + str(tn) + ' fp ' + str(fp) + ' fn ' + str(fn) + ' tp ' + str(tp))\n",
    "    print('\\n------------------------------------\\n')\n",
    "    f.write('\\n------------------------------------\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-c1aaeb87bc00>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-c1aaeb87bc00>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    dict length: 25515\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "frequency is 5\n",
    "dict length: 25515\n",
    "Recall\t\tPrecision\tAccuracy\n",
    "99.83\t\t93.96\t\t97.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB = MultinomialNB()\n",
    "NB.fit(train_dataframe, label_train)\n",
    "\n",
    "NB_result = NB.predict_proba(test_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
